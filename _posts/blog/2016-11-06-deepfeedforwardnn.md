---
layout: post
title: 深度学习：深度前馈网络
description: 深度学习
category: blog
---

## 一 如何理解

 前馈网络之所以称为网络，是因为它们是由多种不同函数组成。网络模型可以看做一个有向无环图，该图描述了函数之间是如何关联的。
 例如，我们有三个函数 $f^{(1)},f^{(2)},f^{(3)}$ 链式相连，形成 $f(x) =f^{(3)}(f^{(2)}(f^{(1)}))$ ，这种链式结构是神经网络中最常用的结构。其中 $f^{(1)}$ 被称为网络的第一层，$f^{(1)}$ 称为第二层...链式的长度即为网络的深度，网络的最后一层为输出层，这也是深度学习的来源。

 网络训练过程中，我们让 $f(x)$ 逐渐匹配 $f^*(x)$ ( $f^*(x)$ 为我们需要拟合的函数) 。训练样本提供了由  $f^*(x)$ 在不同点的带有噪音的近似的样本。每个样本 $x$ 都有一个相对应的标签 $y\sim f^*(x)$  ，训练样本直接指明了输出层在每个点 $x$ 的行为，它应该输出一个近似于 $y$ 的输出。

## 二 示例：学习XOR

  我们通过一个学习XOR函数的任务来让前馈网络的概念更具体。XOR函数是一个在两个二值变量 $x_1,x_2$ 上的操作。当 $x_1,x_2$ 中的某一个值为1时，XOR函数返回1，否则返回0。XOR函数提供了一个目标函数 $y=f^*(x)$ 。我们的模型提供了函数 $y=f(x;\theta)$ ，我们的学习算法将会逐步调整参数 $\theta$  来使得 $f$ 尽可能地趋近于 $f^*$。

  此实例中，我们的目标是使得网络能够正确的预测变量 $x_1,x_2$ 在全部的域值空间的四个点 $X=\{[0,0]^T,[0,1],[1,0],[1,1]\}$。可以将此问题看做回归问题，选择均方误差作为损失函数，在全部训练数据集中，MSE(均方误差)损失函数如下:

  $$
    J(\theta) = \frac{1}{4}\sum_{x\epsilon X}(f^*(x)-f(x;\theta))^2 \tag {6.1}
  $$

### 2.1 模型

  此时我们需要选择模型的形式 $f(x;\theta)$。假若我选择了一个线性模型，其中 $\theta$ 由 $w$ 和 $b$ 组成。我们的模型定义如下

   $$
     f(x;w,b) = x^Tw+b
   $$

将四个训练样本带入可求解得 $w=0,b=1/2$ 此时的线性模型将一直输出0.5。 下图演示了线性模型无法表述 XOR函数。

![](/images/blog/dnn1.png)

图中四个点是学习函数需要在四个坐标输出的值（左下点坐标是(0,0)，要输出0），从左边可以看出线性模型无法实现XOR功能。图右是被神经网络抽取特征的变换空间，此时线性模型可以解决XOR问题了。在样本解中，输出值为1的两个点在特征空间中合并为一个点了

下图演示了一个包含了一个隐藏层（有两个神经元）的网络(之前只有两层，输入和输出)，隐藏层神经元的作用函数 $h$ 为 $f^{(1)}(x;w,c)$ ,隐藏单元的输出将作为第二层的输入。输出层依然是一个线性回归模型，但是它作用对象是 $h$  而不是之前的直接的 $x$ 了。

![](/images/blog/dnn2.png)

网络此时包含两个链式函数，$h=f^{(1)}(w;w,c)$ 和 $y=f^{(2)}(h;w,b)$ 。完整的模型可以表述为:

$$
   f(x;W,c,w,b) =f^{(2)}(f^{(1)}(x))
$$

此时，函数 $f^{(1)}$ 该如何计算呢。如果使用线性模型，将会导致整个网络都沦为输入的线性函数（ $f^{(2)}$(x) 是线性回归）。假若 $f^{(1)}(W^Tx)$ 而 $f^{(2)}(h)=h^Tw$ ，此时整个网络可以表述为 $f(x)=w^TW^Tx$  依然是线性的。

很显然，我们必须使用一个非线性函数来描述这些特征。大多数神经网络是通过一个由学习参数控制的映射转换，加上一个固定的称为激活函数的非线性函数。大多数网络选用 ReLU作为激活函数

### 2.2 ReLU激活函数

 ReLU作为激活函数的依据是，生物的稀疏激活性。人脑神经元同时被激活的只有1%-4%，这可提高学习精度，更好更快地提取稀疏特征。ReLU的**非线性**来源于神经元的部分选择性激活。

 使用ReLU的另一个原因是，Sigmod（另外一种激活函数）网络会出现梯度消失情况。误差反向传播时，梯度

 $$
   Gradient = (y-y^{'})sigmod^{'}*x \\
   sigmod^{'}(x) \quad\quad\epsilon (0,1) \\
   x \quad \epsilon (0,1)
 $$

 会成倍衰减（ $sigmod^{'}(x) \quad\epsilon (0,1)$ ,$x\epsilon (0,1)$）。而ReLU的梯度为 $max\{0,W^Tx+b\}$ 始终为1，只有一端饱和，梯度易流动，同时提高了训练速度。

 关于ReLU的详细可参考 [ReLU激活函数](http://www.cnblogs.com/neopenx/p/4453161.html)

## 三 基于梯度的学习

 神经网络的非线性将导致损失函数非凸，没法全局收敛。应用于非凸的损失函数的SGD（随机梯度下降）方法也无法全局收敛，而且对初始值敏感。对于**前馈网络而言，初始权重应随机初始化为很小的值，偏置 $b$ 初始化为0，或很小的值**。 梯度下降方法应用在各种网络中，用来最小化损失函数。其实在训练SVM，线性回归模型中都可以使用梯度下降方法

### 3.1 损失函数

设计神经网咯时，一个重要的组成部分是损失函数的选择，尽管大部分神经网络的损失函数都与参数模型相似。

大多数情况下，参数模型的定义了一个概率分布 $p(y\|x;\theta)$ ,我们使用的是最大似然定理（深度学习，机器学习的本质，将训练时最优的参数当做实际最优的参数），即，使用训练数据和模型预测值的交叉熵作为损失函数。

交叉熵定义：

$$
  c = -\frac{1}{n}\sum_{x}[ylna+(1-y)ln(1-a)]
$$

交叉熵特性:

+ 非负性
+ 期望与实际误差小时，损失值小，反之则大。

有时候我们会使用一种简化解，不使用全部的 $y$的概率分布，仅预测满足一定条件的x对应的输出y的概率分布。

#### 3.1.1.1 学习条件最大似然的条件分布

 使用最大似然方法训练网络，其损失函数即负的对数似然，可定义为

$$
  J(\theta) = -E_{x,y\sim p^{\sim}_{data}}logp_{model}(y\|x)
$$

不同的参数模型，具体的损失函数不一样。

线性模型的输出的概率分布的最大似然估计等价于最小均方误差（损失函数的一种），实际上，这种等价是以忽略 $f(x;\theta)$ 对高斯分布的均值预测为前提的。

设计神经网络时的一个常见问题是：梯度需要足够大，且易于预测。饱和函数（上面一章中ReLU参考文章中有解释饱和函数）会破坏这一期望，它会将梯度变得特别小，这会导致网络很难调整。如果网络输出层是 $e^x$ 类的函数，若 $x$ 为较小负数，也会饱和。在这样的函数上应用对数函数可以有效化解饱和问题（对数和 $e^x$ 相互抵消）。

交叉熵损失函数可用来完成类似的最大似然估计的功能，因为它没有最小值，但可以无限逼近于0或1，这可同时用于离散化输出，logistic回归就是一个类似的模型。**问题**：若模型可控制输出分布的密度，就有可能给正确的训练集极高的密度，这会使得交叉熵趋于无穷大，使用正则化可用来避免这个问题。

#### 3.1.1.2 学习条件概率

相比于 $p(y\|x,\theta)$ 的完全概率分布，我们一般仅学习一个给定$x$ 的 $y$ 的条件概率分布。假若我们使用一个足够强大的网络，我们可以认为网络能够表述任意函数 $f$ (函数有足够多的分类),此时函数的分类仅受限于特征，比如特征的连续性和无界性，而不是某种固定的参数形式。从这个视角来看，我们可以将损失函数视为**函数式**而非简单的**函数**。**函数式**指的是从函数到实数的映射，而学习过程将会是选择一个函数，而不是一系列参数。此时，我们设计**损失函数式**为在某些期望函数上才取得最小值。比如，我们设计的损失函数式取得最小值的时候，对于给定的特征 $x$ 刚好输出了期望的 $y$ 。

### 3.2 输出神经元

 损失函数的选择与输出神经元的选择直接相关。下文简述了一些不同输出分布时对应的神经元。

#### 3.2.1 高斯分布输出的线性神经元

 对于给定特征 $h$ ，线性输出神经元的输出为 $\bar y =W^Th+B$ 。线性输出层通常用来生产条件高斯分布的均值，此时求对数似然的最大值等价于最小化均方误差。(方差越小，分布越集中)。

 最大似然框架易于学习高斯分布的方差，使得高斯分布的协方差成为输入的一个函数。但是所有的输入其输出必须为正的有限的矩阵。

 #### 3.2.2 Bernoulli输出分布的Sigmoid神经元

许多二分类问题都可以使用Sigmoid神经元输出。最大似然估计的方法是定义一个在条件 $x$ 上的 $y$ 的Bernoulli分布。

定义Bernoulli 分布只需要一个参数（概率），神经网络只需预测概率 $P(y=1\|x)$ 即可。假若使用线性激活神经元，网络输出的预测概率

$$
  P(y=1\|x) =max \lbrace 0,min\lbrace 1,w^Th+b\rbrace \rbrace
$$

其中 $w^Th+b$ 很容易就不在区间 [0,1]上，为0.梯度为0，如何保证一直有较强的梯度呢？我们选用 Sigmoid 作为
