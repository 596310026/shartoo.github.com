---
layout: post
title: 深度学习：参数正规化
description: deep learning基础
category: blog
---

# 第七章 深度学习中的正规化
## 7.1 概念

  深度学习中用以减小测试误差，但可能会增加训练误差的策略称为正规化。

  **限制**：有些正规化策略是机器学习模型上添加限制。有些在模型参数上，有些在目标函数上添加额外项。有些限制或惩罚被设计来对特定先验知识编码的，其余的则是为了提高模型泛化能力。

  深度学习中大部分正规化策略都是基于估计正规化，而估计正规化则是通过增加偏置来减少方差。(>比如，神经网络中的神经元都有一个偏置项b<)。 一个估计的正规化的目标是在大幅度减小方差的同时，尽可能小的带来偏置的增加。我们在讨论泛化和过拟合问题时会遇到以下三种情形:

  ![拟合情况](/images/blog/regular1.png)

  + **欠拟合**:实际的正例没有完全被包含在模型预测域。
  + **绝佳**：完全匹配了数据生成过程.(>完全匹配了模拟的函数<)
  + **过拟合**：模型的预测域包含了全部的正例，同时也包含了负例（）。

  正规化的目标就是将模型的**过拟合**情形改善至**绝佳**情形。

  现实情况中，即便是极端复杂的模型也没法完全拟合目标函数(>让神经网络学习人对图像的认知能力，模型已经达到一千层，参数几千万<)，因为大部分情况，我们并不知道目标函数具体是如何映射的。
  这表明设计一个模型的复杂度极高(模型大小，参数等)。而在近些年的实际实验中，我们发现比较好的模型都是正规化处理过后的大模型。

## 7.2 参数规范惩罚

 目前许多正规化方法，如神经网络、线性回归、logistic回归通过在目标函数$J$上加一个参数规范惩罚项 $\Omega(\theta)$ 公式如下:
 $$
    \bar{J}(\theta;X,y) = J(\theta;X,y)+\alpha\Omega(\theta)\\\tag {7.1}

    其中 \alpha\epsilon [0,\infty)
 $$
 其中，更大的 $\alpha$ 对应更强的正规化处理。

 在神经网络中，使用参数规范惩罚，只是对每一层映射转换的权重，不对偏置使用。这是因为权重决定了两个变量如何交互(>神经元如何输出<)，而偏置只作用于单一变量。同时正规化偏置容易引入欠拟合。

**预定义**

+ 向量$w$代表所有需要被规范惩罚的权重
+ 向量 $\theta$代表所有参数，包括$w$和其他非正规化的参数

 尽管每一层使用独立的 $\alpha$参数的惩罚机制效果可能会更好，但是由于计算量太大。实际中所有层使用相同的权重衰减。

 ### 7.2.1 $L^2$ 参数正规化

   最简单最常见的正规惩罚莫过于$L^2$，有时候称为*权重衰减*，同时也称为*岭回归*或者*吉洪诺夫正规*。它是直接在目标函数后面添加一个正规项 $\Omega(\theta)=\frac{1}{2}||w||^2_2$

   回头看式子 $(7.1)$，假设没有偏置参数，因此 $\theta$参数就是$w$,模型目标函数如下:
   $$
    \bar J(w;X,y) =\frac{\alpha}{2}w^Tw+J(w;X,y) \tag{7.2}
   $$
   对应的参数梯度如下:
   $$
     \bigtriangledown\bar J(w;X,y)=\alpha w+\bigtriangledown_wJ(w;X,y)
   $$
   使用梯度更新权重时:
   $$
    w\leftarrow w-\epsilon(\alpha w+\bigtriangledown_wJ(w;X,y))
   $$
   重写为:
   $$
    w\leftarrow w-(1-\epsilon\alpha)w -\epsilon\bigtriangledown_wJ(w;X,y))
   $$
   可以看到新增权重衰减项最终反映出，它通过对每一步的权重向量乘以一个常数因子修改了学习规则。

   为简化分析，我们假设没有进行正规化处理的训练网络，目标函数(损失函数)最小时对应的权重向量为$w^*$。通过在$w^*$附近某点对目标函数进行二次近似（目标函数得是二次的才可以）。例如线性回归模型的损失函数为均方差，其正规化的方程为:
   $$
   \bar J(\theta)= J(w^*)+\frac{1}{2}(w-w^*)^TH(w-w^*)\\
   其中H为w=w^* 处的Hession矩阵
   $$

   >Hession矩阵是一个多远函数的二阶偏导数构成的方阵，描述的是函数局部曲率。若多元函数在临界点处的一阶梯度为0，仅通过一阶导数无法判断是极大值还是极小值。此时：
   + 若H(M)是**正定**矩阵，则M处为局部极**小**值。
   + 若H(M)是**负定**矩阵，则M处为局部极**大**值。
   + 若H(M)是**不定**矩阵，则M处为不是极值。

  正定矩阵的定义：设M是n阶方阵，如果对任何非零向量z，都有$z^TMz>0$，其中$z^T$ 表示z的转置，就称M正定矩阵
