---
layout: post
title: 深度学习：参数正规化
description: deep learning基础
category: blog
---

## 7.1 概念

  深度学习中用以减小测试误差，但可能会增加训练误差的策略称为正规化。

  **限制**：有些正规化策略是机器学习模型上添加限制。有些在模型参数上，有些在目标函数上添加额外项。有些限制或惩罚被设计来对特定先验知识编码的，其余的则是为了提高模型泛化能力。

  深度学习中大部分正规化策略都是基于估计正规化，而估计正规化则是通过增加偏置来减少方差。(>比如，神经网络中的神经元都有一个偏置项b<)。 一个估计的正规化的目标是在大幅度减小方差的同时，尽可能小的带来偏置的增加。我们在讨论泛化和过拟合问题时会遇到以下三种情形:

  ![拟合情况](/images/blog/regular1.png)

  + **欠拟合**:实际的正例没有完全被包含在模型预测域。
  + **绝佳**：完全匹配了数据生成过程.(>完全匹配了模拟的函数<)
  + **过拟合**：模型的预测域包含了全部的正例，同时也包含了负例（）。

  正规化的目标就是将模型的**过拟合**情形改善至**绝佳**情形。

  现实情况中，即便是极端复杂的模型也没法完全拟合目标函数(>让神经网络学习人对图像的认知能力，模型已经达到一千层，参数几千万<)，因为大部分情况，我们并不知道目标函数具体是如何映射的。
  这表明设计一个模型的复杂度极高(模型大小，参数等)。而在近些年的实际实验中，我们发现比较好的模型都是正规化处理过后的大模型。

## 7.2 参数规范惩罚

 目前许多正规化方法，如神经网络、线性回归、logistic回归通过在目标函数$J$上加一个参数规范惩罚项 $\Omega(\theta)$ 公式如下:
$$
    \bar{J}(\theta;X,y) = J(\theta;X,y)+\alpha\Omega(\theta)\\\tag {7.1}

    其中 \alpha\epsilon [0,\infty)
$$
其中，更大的 $\alpha$ 对应更强的正规化处理。

 在神经网络中，使用参数规范惩罚，只是对每一层映射转换的权重，不对偏置使用。这是因为权重决定了两个变量如何交互(>神经元如何输出<)，而偏置只作用于单一变量。同时正规化偏置容易引入欠拟合。

**预定义**

+ 向量$w$代表所有需要被规范惩罚的权重
+ 向量 $\theta$代表所有参数，包括$w$和其他非正规化的参数

 尽管每一层使用独立的 $\alpha$参数的惩罚机制效果可能会更好，但是由于计算量太大。实际中所有层使用相同的权重衰减。

### 7.2.1 $L^2$ 参数正规化

最简单最常见的正规惩罚莫过于$L^2$，有时候称为*权重衰减*，同时也称为*岭回归*或者*吉洪诺夫正规*。它是直接在目标函数后面添加一个正规项 $\Omega(\theta)=\frac{1}{2}\|\|w\|\|^2_2$

L2范数是指向量各元素的平方和然后求平方根。我们让L2范数的规则项 $\|\|W\|\|_2$最小，可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0。

回头看式子 $(7.1)$，假设没有偏置参数，因此 $\theta$参数就是$w$,模型目标函数如下:
   $$
    \bar J(w;X,y) =\frac{\alpha}{2}w^Tw+J(w;X,y) \tag{7.2}
   $$
对应的参数梯度如下:
   $$
     \bigtriangledown\bar J(w;X,y)=\alpha w+\bigtriangledown_wJ(w;X,y)
   $$
使用梯度更新权重时:
   $$
    w\leftarrow w-\epsilon(\alpha w+\bigtriangledown_wJ(w;X,y))
   $$
   重写为:
   $$
    w\leftarrow w-(1-\epsilon\alpha)w -\epsilon\bigtriangledown_wJ(w;X,y))
   $$
可以看到新增权重衰减项最终反映出，它通过对每一步的权重向量乘以一个常数因子修改了学习规则。由于 $\epsilon$ ，$\alpha$都是正值，所以它实际是减小了$w$。这就是它被称为**权重衰减**的原因。

 **如何防止过拟合**:更小的权值w，从某种意义上说，表示网络的复杂度更低，对数据的拟合刚刚好,而在实际应用中，也验证了这一点，L2正则化的效果往往好于未经正则化的效果。

 过拟合的时候，拟合函数的系数往往非常大，为什么？如下图所示，过拟合，就是拟合函数需要顾及每一个点，最终形成的拟合函数波动很大。在某些很小的区间里，函数值的变化很剧烈。这就意味着函数在某些小区间里的导数值（绝对值）非常大，由于自变量值可大可小，所以只有系数足够大，才能保证导数值很大。

 ![L2正则化示例](/images/blog/regular2.png)

 而正则化是通过约束参数的范数使其不要太大，所以可以在一定程度上减少过拟合情况。



### 7.2.2 $L^1$正规化处理

对模型参数$w$上的$L^1$正归化是增加一个惩罚项
$$
  \Omega(\theta) = \|\|w\|\|_1=\sum_i\|w_i\|
$$
完整公式如下：
$$
     \bar J(w;X,y) =\alpha\|\|w\|\|_1+J(w;X,y)
$$
 对上式求梯度得到：
 $$
 \bigtriangledown _w \bar J(w;X,y) = \alpha sign(w)+\bigtriangledown _w J(X,y;w)
 $$
 可以看到，$L^1$正规化对梯度的影响不再与每个$w_i$线性相关，而是一个常量因子。符号只与$w$相关，当w为正时，更新后的w变小。当w为负时，更新后的w变大——因此它的效果就是让w往0靠，使网络中的权重尽可能为0，也就相当于减小了网络复杂度，防止过拟合。

 当w为0时怎么办？当w等于0时，|W|是不可导的，所以我们只能按照原始的未经正则化的方法去更新w，这就相当于去掉 $\alpha sign(w)$ 这一项，所以我们可以规定$sgn(0)=0$，这样就把w=0的情况也统一进来了
