---
layout: post
title: 深度学习：卷积神经网络与图像识别基本概念
description: 神经网络基础
category: blog
---
## 一 卷积神经网络的组成

 图像分类可以认为是给定一副测试图片作为输入 $I \epsilon R^{W×H×C}$，输出该图片
属于哪一类。参数 W 是图像的宽度，H 是高度，C 是通道的个数；彩色图像中 C = 3，灰度图像
中 C = 1。一般的会设定总共类别的个数，例如在ImageNet竞赛中总共有 1000 个类别；在CIFAR10[14] 中有 10 个类别。卷积神经网络则可以看成这样的黑匣子。输入是原始图片 I，输出是 L 维的向量 $v \epsilon R^L$。L表示预先设定的类别个数。向量 v 的每一个维度代表图像属于对应类别的可能性的大小。如果是
单类别识别问题，也就是说每一幅图像只分配 L 个标签中的一个标签，那么可以对 v 中的元素进行比较，选取最大的值对应的标签作为分类的结果。v 可以是一个概率分布的形式，即每一个元素$0 ≤ vi ≤ 1$，并且 $\sum_iv_i=1$ 。其中 $v_i$ 表示 v 的第 i 个元素。也可以是从负无穷大到正无穷大的实数，越大代表属于对应类别的可能性越大。在卷积神经网络的内部，是由很多的层构成。每一个层可以认为是一个函数，输入是信号 x，输出是信号 $y = f(x)$ 。输出的 y 又可以作为其他层的输入。以下从网络的前段，中端，末端的角度调研常用的层的定义。前端主要考虑对于图像的处理过程，中端是各种神经元，末端主要考虑与训练网络有关的损失函数。

### 1.1 网络的前段

  前段指 的是对图像数据的处理，可以称之为数据层。

#### 1.1.1 数据裁减

 输入的图像的大小可能各不相同，有一些图像的分辨率较大，有一些比较小。而且长宽比也不一定会一样。对于这样的不一致性，理论上而言，可以不予处理，但是这要求网络中其他的层次支持这样的输入。目前大部分情况下采用的是通过裁剪的方法使得输出的图像是固定分辨率的。
 在网络训练的阶段，裁剪的位置从原始的图像上随机选择，只需要满足裁剪
的子图完全落在图像中即可。通过随机的方式，是因为相当于增加了额外的数据，能够缓解过拟合的问题。

### 1.1.2 颜色干扰

 裁剪之后的原图，每一个像素的是 0 到 255 的固定的数值。进一步的处理，包括减去均值，以及等比例缩放像素值使得像素值的分部基本在 [−1, 1] 之间。除了这些常规的操作之外，也会对图像进行归一化，相当于图像增强，比如 [9, 18, 17] 中对 CIFAR10 的数据预处理中。比如，对于每一个像素，随机选择 RGB 三个通道中的一个，然后在原像素值的基础上，随机添加一个从 [-20,20] 之间的数值。


### 1.2 网络的中段

  以下介绍在卷及神经网络中常用的层的定义，即输入的数据 x 是什么维度，输出 y 是什么维度以及如何从输入得到输出。

### 1.2.1 卷积神经网络的基本组成

 如下图：

  ![卷积神经网络基本组成](/iamges/blog/cnn_consist.png)

#### 1.2.2 卷积层

  卷积层输入表示为 $x \epsilon R^{W\times H\times C}$,是一个三维的数据。表示有C个矩阵,每个矩阵这里表示为 $x^c \epsilon R^{W\times H}$,也称之为特征图。输出 $y \epsilon R^{W_0\times H_0\times C_0}$，也是一个三维数据。特征图分辨率从$W\times H$变为$W_0\times H_0$,特征图的个数也从C变为$C_0$。
    从输入到输出的一般公式为：
  $$
     y^{c_1}=\sum_cX^c*W^{c,c_1}
  $$

  矩阵 $wc,c_1\epsilon R_{w\times h}$ 称之为卷积核。属于卷积层的参数，一般通过随机梯度下降更新。$x^c$ 为输入数据的第 c 个特征图，但在一些情况下，也会在图像的周围补白。符号 ∗ 表示二维数据的卷积运算。卷积定义为
  $$
    (X^c*W^{c,c_1})=\sum_{m,n}x^c_{m,n}w^{c,c_1}_{u-m,v-n}
  $$
符号 $()_{u,v}$ 表示对应矩阵的 u 行 v 列的元素值。在有一些的网络结构中，并不是选择所有的 (u, v)，而是每隔一定数量选择一个。
直观而言，卷积层相当于对图像进行滤波，希望能够抽象出来局部信息。局部信息通过较小的卷积核在图像不同的局部位置上扫描而得。
![卷积层计算](/images/blog/cnn_compute.png)

下图是是一个动态示例，来源于 [convolutional-networks](https://cs231n.github.io/convolutional-networks/#conv)


#### 1.2.3 池化层

输入的信号表示为 $x\epsilon R^{W\times H\times C}$，具有 C 个通道，每一个通道是一个特征图。输出 $y\epsilon R^{W_0\times H_0\times C}$ 具有的通道个数与输入相同，但是特征图的分辨率一般是降低。

池化层是对每一个特征图单独进行操作并且输出一个对应的特征图。假设池化范围是 $w \times h$，那么输入的特征图提取出来 $w \times h$ 的小图，然后寻找子图的最大值，或者计算子图的均值，作为一个输出。签证一般称之为最大化池化，后者是均值池化。从图像中提出小图的方式可以是任意一个子图，也可以是每隔多个像素值得到一个子图。池化层的作用包括降低特征图的分辨率，从而减少计算量，以及增强网络的鲁棒性。比如对于最大化池化的方式，对于图像的平移具有一定的鲁棒性。

池化层的作用包括降低特征图的分辨率，从而减少计算量，以及增强网络的鲁棒性。比如对于最大化池化的方式，对于图像的平移具有一定的鲁棒性。

实例，对于如下特征图 4x4，使用最大池化效果如下：
![池化](/images/blog/cnn_maxpool.png)
图中每个像素点的值是上面各个格子的数值，然后要对这张 4*4的图片进行池化；那么采用最大池化也就是对上面 4*4的图片分块，每块大小为2*2，然后统计每个块的最大值，作为下采样后图片的像素值。

#### 1.2.4 CCCP

CCCP层的输入是$x\epsilon R^{W\times H\times C}$，输出是$y\epsilon W\times H\times C$。特征层的分辨率保持不变，但是通道数有所改变。其定义为：
$$
   y^{c_0}_{u,v}=\sum_cx^c_{u,v}w^{c,c_0}
$$
等效于卷积核为 1x1的卷积层。
CCCP 层相当于在多个全连接层，每一个全连接将信号从 C 维度映射为$C_0$维度。

#### 1.2.5 ReLU 层及相关变体

该层的输入认识是一个信号 x。ReLU 并不要求输入信号的维度必须是一维或者几维的，因为该层的操作是对输出的每一个元素单独操作。但依然可以认为输入的 $x\epsilon R^{W\times H\times C}$。输出是一个和输入维度一样的信号y。
   假设从输入到输出的一个示例为:
   $$
 y_i =
\begin{cases}
	x_i,  & \text{if $x_i\ge 0$ } \\
	0, & \text{if $x_i<0$ }  \\
\end{cases}
$$

显然这是一个非线性操作，ReLU 的存在使得网络的表达更加丰富。同时从定义中容易得出，该操作非常简单，并且在不同的输入点之间进行并行。ReLU 在一定程度上也是 S 行函数的近似。
$$
   y_i=\frac{1}{1+e^{-x_i}}
$$
进一步将ReLU改进为：
$$
y_i =
\begin{cases}
x_i,  & \text{if $x_i\ge 0$ } \\
0.01x_i, & \text{if $x_i<0$ }  \\
\end{cases}
$$
当元素值为负数的时候，通过 $y_i = 0.01x_i$ 的方式，避免了导数为 0，无法传播的情况。
 进一步使用修正的ReLu为：
 $$
   y_i=
   \begin{cases}
   x_i,  & \text{if $x_i\ge 0$ } \\
   \alpha x_i, & \text{if $x_i<0$ }  \\
   \end{cases}
 $$
 其中斜率 $\alpha$ 不再是一个固定的数值，而是通过梯度下降的方式就行优化

 #### 1.2.5 Dropout层

 Dropout层的输入为$x\epsilon R^{W\times H\times C}$。这里并不要求输入是三维的信号，任意可能的维度都是可以。Dropout 同样是针对每一个数据进行操作。输出 y 与输入的大小一致。在网路进行训练的时候，对于输入的每一个数值 xi，按照概率 p 设置为 0，否则保留。数学形式可以写为:
   $$y_i=\epsilon x_i$$
   其中 $\epsilon$ 是随机变量，并且满足 $\epsilon = 0$的概率为 p，$\epsilon = 1$ 的概率为 1 − p。实际中，概率 p 往往设置
为 1。

然而在进行测试的时候，计算公式更正为 $y_i=(1-p)x_i$相当于一个期望。

Dropout层的引入主要是为了减少过拟合的问题，减少不同参数的耦合性。

#### 1.2.6 全连接层

输入时$x\epsilon R^D$。这里要求将输入认识是一个列向量。输出为 $y\epsilon R^P$。从输入到输出的关系是:
$$
   y= Wx+b
$$
其中$W\epsilon R^{P\times D},b\epsilon R^P$,是投影矩阵阵以及阈值，是该层的参数，通过随机梯度下降的方式更新优化。

全连接层是一个非常常用的层，然而该层在一定程度上会损失图像的空间信息，故而在有一些网络中，抛弃了全连接层。
